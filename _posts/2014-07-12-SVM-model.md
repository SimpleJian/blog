---
layout: post
title: 最优间隔分类器建模
tag: SVM, classification, model
---

# {{ page.title }}

针对任何分类问题，通过特征提取后，就能得到特征向量及对应的类别标签：

$$
( {\vec x}_i, y_i ), \quad i=1,2,3 \cdots m
$$

其中，$$ {\vec x}_i $$表示第i个样本的特征向量，$$ y_i $$表示第i个样本所属类别。

对于二分类问题，设线性分类器的判别函数为 $$ d({\vec x})={\vec w} \cdot {\vec x}+b $$，对于一个待分类样本，通过特征提取得到其特征向量，将特征向量代入分类器判别函数，若输出为正，则将样本判定为+1类，否则为-1类。从几何的观点来看，特征提取是将样本抽象为欧氏空间中的一个点，而判别函数是该空间中的一个超平面，超平面不同侧的点对应不同类别的样本。因此，如果样本线性可分的话，那么就存在无穷多个超平面将其分开，也即存在无穷多个线性分类器能将样本分开，既然是这样，那么哪一个最好呢？直观上看，应该使分类超平面离样本点越远越好，这样可以减小把新样本分错的概率。当然，此处的“越远越好”的前提是要正确分类。分类器的间隔定义为所有样本点到分类超平面的距离中的最小值。最优间隔分类器就是间隔最大的分类器。

样本点$$ {\vec x}_i $$到超平面的距离为：

$$
\frac{|\vec w \cdot {\vec x}_i + b|}{\Vert w \Vert}
$$

最优间隔分类器的优化目标为：

$$
\mathop{max}_{\vec w, b} \quad \mathop{min}_i \{ \frac{|\vec w \cdot {\vec x}_i + b|}{\Vert w \Vert} \}
$$

要直接求解上述优化问题显然不易，然而，幸运的是可以将其转化为一个凸优化问题。
